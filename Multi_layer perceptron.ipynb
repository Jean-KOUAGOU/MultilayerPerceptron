{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return torch.tanh(x)\n",
    "    \n",
    "def dsigma(x):\n",
    "    return 1-torch.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y):\n",
    "    return torch.norm(x-y)**2\n",
    "\n",
    "def dloss(x,y):\n",
    "    return 2*(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w1, b1, w2, b2, x):\n",
    "    s1=torch.mm(w1, x)+b1\n",
    "    x1=sigma(s1)\n",
    "    s2=torch.mm(w2, x1)+b2\n",
    "    x2=sigma(s2)\n",
    "    return x, s1, x1, s2, x2\n",
    "    \n",
    "def backward_pass(w1, b1, w2, b2, t, x, s1, x1, s2, x2):\n",
    "    dl_dx2=dloss(x2, t)\n",
    "    ds2_dx1 = w2.T\n",
    "    dx1_ds1 = torch.diag(dsigma(s1).squeeze())\n",
    "    dx2_ds2 = torch.diag(dsigma(s2).squeeze())\n",
    "    dl_ds2=torch.mm(dx2_ds2, dl_dx2)\n",
    "    dx2_dx1 = torch.mm(ds2_dx1, dx2_ds2)\n",
    "    dl_dx1 = torch.mm(dx2_dx1, dl_dx2)\n",
    "    dl_ds1 = torch.mm(dx1_ds1, dl_dx1)\n",
    "    ds1_dw1 = x.T\n",
    "    dl_dw1 = torch.mm(dl_ds1, ds1_dw1)\n",
    "    ds2_dw2 = x1.T\n",
    "    dl_dw2 = torch.mm(dl_ds2, ds2_dw2)   \n",
    "    dl_db1 = dl_ds1\n",
    "    dl_db2 = dl_ds2\n",
    "    return dl_dw1, dl_dw2, dl_db1, dl_db2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, s1, x1, s2, x2 = forward(w1, b1, w2, b2, train_input[0].unsqueeze(1))\n",
    "# t=torch.randn(x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward_pass(w1, b1, w2, b2, t, x, s1, x1, s2, x2, dl_dw1, dl_db1, dl_dw2, dl_db2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/anaconda3/envs/NewEnv1/lib/python3.7/site-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/aims/anaconda3/envs/NewEnv1/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/aims/anaconda3/envs/NewEnv1/lib/python3.7/site-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/aims/anaconda3/envs/NewEnv1/lib/python3.7/site-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "from  dlc_practical_prologue import load_data\n",
    "train_input, train_target, test_input, test_target = load_data(one_hot_labels = True, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target=train_target*0.9\n",
    "test_target=test_target*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f304510ccd0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input.unsqueeze(1)[14].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x):\n",
    "    idx=x.argmax(dim=1)\n",
    "    pred=torch.zeros_like(x)\n",
    "    pred[torch.arange(x.size(0)), idx]=0.9\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/1000, training loss : 0.810, test loss : 0.810, accu_train : 9.90%, accu_test : 11.60%\n",
      "Epoch : 10/1000, training loss : 0.739, test loss : 0.731, accu_train : 11.70%, accu_test : 9.90%\n",
      "Epoch : 20/1000, training loss : 0.739, test loss : 0.730, accu_train : 11.70%, accu_test : 9.90%\n",
      "Epoch : 30/1000, training loss : 0.739, test loss : 0.729, accu_train : 26.20%, accu_test : 21.60%\n",
      "Epoch : 40/1000, training loss : 0.731, test loss : 0.635, accu_train : 32.50%, accu_test : 29.50%\n",
      "Epoch : 50/1000, training loss : 0.497, test loss : 0.598, accu_train : 53.40%, accu_test : 47.70%\n",
      "Epoch : 60/1000, training loss : 0.453, test loss : 0.584, accu_train : 60.80%, accu_test : 57.60%\n",
      "Epoch : 70/1000, training loss : 0.399, test loss : 0.630, accu_train : 73.70%, accu_test : 68.80%\n",
      "Epoch : 80/1000, training loss : 0.329, test loss : 0.719, accu_train : 78.70%, accu_test : 72.70%\n",
      "Epoch : 90/1000, training loss : 0.307, test loss : 0.669, accu_train : 81.50%, accu_test : 70.50%\n",
      "Epoch : 100/1000, training loss : 0.179, test loss : 0.692, accu_train : 85.40%, accu_test : 76.00%\n",
      "Epoch : 110/1000, training loss : 0.508, test loss : 0.771, accu_train : 83.00%, accu_test : 73.40%\n",
      "Epoch : 120/1000, training loss : 0.208, test loss : 0.831, accu_train : 88.00%, accu_test : 78.10%\n",
      "Epoch : 130/1000, training loss : 0.269, test loss : 0.816, accu_train : 85.20%, accu_test : 74.10%\n",
      "Epoch : 140/1000, training loss : 0.135, test loss : 0.951, accu_train : 88.80%, accu_test : 79.50%\n",
      "Epoch : 150/1000, training loss : 0.239, test loss : 0.709, accu_train : 91.10%, accu_test : 80.20%\n",
      "Epoch : 160/1000, training loss : 0.119, test loss : 0.822, accu_train : 89.30%, accu_test : 78.60%\n",
      "Epoch : 170/1000, training loss : 0.254, test loss : 0.782, accu_train : 91.10%, accu_test : 80.20%\n",
      "Epoch : 180/1000, training loss : 0.152, test loss : 0.741, accu_train : 92.70%, accu_test : 81.10%\n",
      "Epoch : 190/1000, training loss : 0.102, test loss : 0.718, accu_train : 91.50%, accu_test : 79.20%\n",
      "Epoch : 200/1000, training loss : 0.148, test loss : 0.763, accu_train : 94.40%, accu_test : 82.80%\n",
      "Epoch : 210/1000, training loss : 0.132, test loss : 0.899, accu_train : 95.00%, accu_test : 84.10%\n",
      "Epoch : 220/1000, training loss : 0.144, test loss : 1.123, accu_train : 92.10%, accu_test : 80.70%\n",
      "Epoch : 230/1000, training loss : 0.209, test loss : 0.954, accu_train : 94.30%, accu_test : 82.60%\n",
      "Epoch : 240/1000, training loss : 0.187, test loss : 1.105, accu_train : 95.00%, accu_test : 82.90%\n",
      "Epoch : 250/1000, training loss : 0.132, test loss : 0.881, accu_train : 95.40%, accu_test : 83.20%\n",
      "Epoch : 260/1000, training loss : 0.141, test loss : 0.703, accu_train : 96.10%, accu_test : 82.50%\n",
      "Epoch : 270/1000, training loss : 0.091, test loss : 0.747, accu_train : 96.20%, accu_test : 83.90%\n",
      "Epoch : 280/1000, training loss : 0.057, test loss : 0.780, accu_train : 95.50%, accu_test : 81.90%\n",
      "Epoch : 290/1000, training loss : 0.092, test loss : 0.976, accu_train : 95.50%, accu_test : 83.70%\n",
      "Epoch : 300/1000, training loss : 0.122, test loss : 1.074, accu_train : 95.80%, accu_test : 83.20%\n",
      "Epoch : 310/1000, training loss : 0.087, test loss : 1.033, accu_train : 96.40%, accu_test : 84.50%\n",
      "Epoch : 320/1000, training loss : 0.134, test loss : 0.800, accu_train : 96.40%, accu_test : 83.10%\n",
      "Epoch : 330/1000, training loss : 0.095, test loss : 0.742, accu_train : 96.50%, accu_test : 82.40%\n",
      "Epoch : 340/1000, training loss : 0.137, test loss : 1.124, accu_train : 96.50%, accu_test : 83.20%\n",
      "Epoch : 350/1000, training loss : 0.135, test loss : 0.987, accu_train : 97.10%, accu_test : 83.60%\n",
      "Epoch : 360/1000, training loss : 0.063, test loss : 0.796, accu_train : 97.50%, accu_test : 83.20%\n",
      "Epoch : 370/1000, training loss : 0.042, test loss : 0.799, accu_train : 97.60%, accu_test : 84.20%\n",
      "Epoch : 380/1000, training loss : 0.099, test loss : 0.881, accu_train : 97.60%, accu_test : 83.90%\n",
      "Epoch : 390/1000, training loss : 0.053, test loss : 1.159, accu_train : 97.10%, accu_test : 84.40%\n",
      "Epoch : 400/1000, training loss : 0.029, test loss : 1.016, accu_train : 97.60%, accu_test : 84.80%\n",
      "Epoch : 410/1000, training loss : 0.043, test loss : 0.744, accu_train : 97.50%, accu_test : 83.50%\n",
      "Epoch : 420/1000, training loss : 0.031, test loss : 0.751, accu_train : 97.50%, accu_test : 83.40%\n",
      "Epoch : 430/1000, training loss : 0.062, test loss : 0.810, accu_train : 97.90%, accu_test : 83.10%\n",
      "Epoch : 440/1000, training loss : 0.058, test loss : 1.012, accu_train : 97.90%, accu_test : 83.50%\n",
      "Epoch : 450/1000, training loss : 0.054, test loss : 0.956, accu_train : 98.10%, accu_test : 83.30%\n",
      "Epoch : 460/1000, training loss : 0.109, test loss : 0.905, accu_train : 98.30%, accu_test : 83.90%\n",
      "Epoch : 470/1000, training loss : 0.045, test loss : 0.913, accu_train : 98.00%, accu_test : 84.50%\n",
      "Epoch : 480/1000, training loss : 0.052, test loss : 0.636, accu_train : 97.90%, accu_test : 83.00%\n",
      "Epoch : 490/1000, training loss : 0.041, test loss : 0.757, accu_train : 98.10%, accu_test : 83.50%\n",
      "Epoch : 500/1000, training loss : 0.040, test loss : 0.809, accu_train : 98.50%, accu_test : 84.20%\n",
      "Epoch : 510/1000, training loss : 0.041, test loss : 0.858, accu_train : 98.60%, accu_test : 84.10%\n",
      "Epoch : 520/1000, training loss : 0.186, test loss : 1.119, accu_train : 97.80%, accu_test : 83.40%\n",
      "Epoch : 530/1000, training loss : 0.044, test loss : 1.010, accu_train : 98.40%, accu_test : 85.00%\n",
      "Epoch : 540/1000, training loss : 0.083, test loss : 0.907, accu_train : 98.40%, accu_test : 84.30%\n",
      "Epoch : 550/1000, training loss : 0.077, test loss : 0.805, accu_train : 98.50%, accu_test : 83.70%\n",
      "Epoch : 560/1000, training loss : 0.033, test loss : 0.738, accu_train : 98.70%, accu_test : 83.70%\n",
      "Epoch : 570/1000, training loss : 0.025, test loss : 0.770, accu_train : 98.80%, accu_test : 84.10%\n",
      "Epoch : 580/1000, training loss : 0.059, test loss : 0.850, accu_train : 98.80%, accu_test : 83.70%\n",
      "Epoch : 590/1000, training loss : 0.070, test loss : 0.858, accu_train : 98.50%, accu_test : 84.10%\n",
      "Epoch : 600/1000, training loss : 0.078, test loss : 1.112, accu_train : 98.50%, accu_test : 85.30%\n",
      "Epoch : 610/1000, training loss : 0.045, test loss : 1.081, accu_train : 98.70%, accu_test : 84.60%\n",
      "Epoch : 620/1000, training loss : 0.060, test loss : 1.137, accu_train : 98.60%, accu_test : 84.80%\n",
      "Epoch : 630/1000, training loss : 0.028, test loss : 1.024, accu_train : 98.80%, accu_test : 84.20%\n",
      "Epoch : 640/1000, training loss : 0.024, test loss : 0.834, accu_train : 99.00%, accu_test : 84.30%\n",
      "Epoch : 650/1000, training loss : 0.040, test loss : 0.779, accu_train : 98.90%, accu_test : 83.70%\n",
      "Epoch : 660/1000, training loss : 0.030, test loss : 0.880, accu_train : 99.10%, accu_test : 83.50%\n",
      "Epoch : 670/1000, training loss : 0.030, test loss : 0.949, accu_train : 98.90%, accu_test : 83.80%\n",
      "Epoch : 680/1000, training loss : 0.059, test loss : 1.024, accu_train : 98.70%, accu_test : 82.50%\n",
      "Epoch : 690/1000, training loss : 0.037, test loss : 0.792, accu_train : 99.10%, accu_test : 82.80%\n",
      "Epoch : 700/1000, training loss : 0.030, test loss : 0.736, accu_train : 99.10%, accu_test : 83.70%\n",
      "Epoch : 710/1000, training loss : 0.029, test loss : 0.797, accu_train : 99.10%, accu_test : 84.00%\n",
      "Epoch : 720/1000, training loss : 0.034, test loss : 0.879, accu_train : 99.10%, accu_test : 84.10%\n",
      "Epoch : 730/1000, training loss : 0.039, test loss : 0.935, accu_train : 99.10%, accu_test : 84.00%\n",
      "Epoch : 740/1000, training loss : 0.048, test loss : 1.000, accu_train : 98.90%, accu_test : 83.40%\n",
      "Epoch : 750/1000, training loss : 0.071, test loss : 1.189, accu_train : 98.70%, accu_test : 83.60%\n",
      "Epoch : 760/1000, training loss : 0.016, test loss : 0.963, accu_train : 99.20%, accu_test : 85.20%\n",
      "Epoch : 770/1000, training loss : 0.041, test loss : 1.048, accu_train : 99.10%, accu_test : 85.00%\n",
      "Epoch : 780/1000, training loss : 0.082, test loss : 1.161, accu_train : 98.90%, accu_test : 85.20%\n",
      "Epoch : 790/1000, training loss : 0.044, test loss : 1.036, accu_train : 99.20%, accu_test : 84.60%\n",
      "Epoch : 800/1000, training loss : 0.051, test loss : 1.050, accu_train : 99.20%, accu_test : 83.30%\n",
      "Epoch : 810/1000, training loss : 0.037, test loss : 1.024, accu_train : 99.00%, accu_test : 84.00%\n",
      "Epoch : 820/1000, training loss : 0.033, test loss : 1.062, accu_train : 99.00%, accu_test : 83.60%\n",
      "Epoch : 830/1000, training loss : 0.015, test loss : 0.991, accu_train : 99.20%, accu_test : 84.50%\n",
      "Epoch : 840/1000, training loss : 0.020, test loss : 0.925, accu_train : 99.30%, accu_test : 83.80%\n",
      "Epoch : 850/1000, training loss : 0.044, test loss : 0.844, accu_train : 99.30%, accu_test : 83.30%\n",
      "Epoch : 860/1000, training loss : 0.045, test loss : 0.865, accu_train : 99.30%, accu_test : 82.90%\n",
      "Epoch : 870/1000, training loss : 0.021, test loss : 0.856, accu_train : 99.30%, accu_test : 83.70%\n",
      "Epoch : 880/1000, training loss : 0.035, test loss : 0.796, accu_train : 99.30%, accu_test : 83.50%\n",
      "Epoch : 890/1000, training loss : 0.049, test loss : 0.768, accu_train : 99.40%, accu_test : 83.70%\n",
      "Epoch : 900/1000, training loss : 0.058, test loss : 0.850, accu_train : 99.30%, accu_test : 84.30%\n",
      "Epoch : 910/1000, training loss : 0.060, test loss : 0.971, accu_train : 99.30%, accu_test : 83.20%\n",
      "Epoch : 920/1000, training loss : 0.036, test loss : 0.939, accu_train : 99.30%, accu_test : 83.70%\n",
      "Epoch : 930/1000, training loss : 0.030, test loss : 0.914, accu_train : 99.40%, accu_test : 83.90%\n",
      "Epoch : 940/1000, training loss : 0.038, test loss : 0.847, accu_train : 99.40%, accu_test : 83.30%\n",
      "Epoch : 950/1000, training loss : 0.050, test loss : 0.791, accu_train : 99.30%, accu_test : 82.50%\n",
      "Epoch : 960/1000, training loss : 0.030, test loss : 0.989, accu_train : 99.20%, accu_test : 82.80%\n",
      "Epoch : 970/1000, training loss : 0.025, test loss : 1.055, accu_train : 99.30%, accu_test : 83.80%\n",
      "Epoch : 980/1000, training loss : 0.015, test loss : 0.923, accu_train : 99.40%, accu_test : 83.90%\n",
      "Epoch : 990/1000, training loss : 0.031, test loss : 0.817, accu_train : 99.50%, accu_test : 83.40%\n"
     ]
    }
   ],
   "source": [
    "eps=1e-6\n",
    "w1=torch.normal(mean=torch.zeros((50, 784)), std=torch.ones((50, 784))*eps)\n",
    "w2=torch.normal(mean=torch.zeros((10, 50)), std=torch.ones((10, 50))*eps)\n",
    "b1=torch.normal(mean=torch.zeros((50, 1)), std=torch.ones((50, 1))*eps)\n",
    "b2=torch.normal(mean=torch.zeros((10, 1)), std=torch.ones((10, 1))*eps)\n",
    "\n",
    "num_epoch=1000\n",
    "lr=1e-4\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    dl_dw1=torch.zeros_like(w1)\n",
    "    dl_dw2=torch.zeros_like(w2)\n",
    "    dl_db1=torch.zeros_like(b1)\n",
    "    dl_db2=torch.zeros_like(b2)\n",
    "    X2=[]\n",
    "    X2_t1=[]\n",
    "    for idx in range(train_input.size(0)):\n",
    "        x=train_input[idx].unsqueeze(1)\n",
    "        t=train_target[idx].unsqueeze(1)\n",
    "        t1=test_target[idx].unsqueeze(1)\n",
    "        x_t1=test_input[idx].unsqueeze(1)\n",
    "        _, s1, x1, s2, x2 = forward(w1, b1, w2, b2, x)\n",
    "        X2.append(x2.T)\n",
    "        _, _, _, _, x2_t1=forward(w1, b1, w2, b2, x_t1)\n",
    "        new_dl_dw1, new_dl_dw2, new_dl_db1, new_dl_db2 = backward_pass(w1, b1, w2, b2, t, x, s1, x1, s2, x2)\n",
    "        X2_t1.append(x2_t1)\n",
    "        dl_dw1 += new_dl_dw1\n",
    "        dl_dw2 += new_dl_dw2\n",
    "        dl_db1 += new_dl_db1\n",
    "        dl_db2 += new_dl_db2\n",
    "        \n",
    "    X2 = torch.stack(X2, dim=0).squeeze()\n",
    "    X2_t1 = torch.stack(X2_t1, dim=0).squeeze()\n",
    "    w1 -= lr*dl_dw1\n",
    "    w2 -= lr*dl_dw2\n",
    "    b1 -= lr*dl_db1\n",
    "    b2 -= lr*dl_db2\n",
    "    if i%10 == 0:\n",
    "        accuracy1 = ((prediction(X2) == train_target).all(dim=1)*1.0).mean()\n",
    "        accuracy2 = ((prediction(X2_t1) == test_target).all(dim=1)*1.0).mean()\n",
    "        print(\"Epoch : {}/{}, training loss : {:.3f}, test loss : {:.3f}, accu_train : {:.2f}%, accu_test : {:.2f}%\".format(i, num_epoch, \\\n",
    "                                                           loss(x2, t), loss(x2_t1, t1), accuracy1*100, accuracy2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        8, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        5, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 2, 7,\n",
       "        1, 6, 3, 0, 2, 1, 1, 7, 0, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "        7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
       "        2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 8, 9, 1, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
       "        8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
       "        7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
       "        0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 7, 2, 7, 3,\n",
       "        0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
       "        8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3, 6, 4, 7, 5, 0, 6, 2, 7,\n",
       "        9, 8, 5, 9, 2, 1, 1, 4, 4, 5, 6, 4, 1, 2, 2, 3, 9, 3, 9, 0, 5, 9, 6, 5,\n",
       "        7, 4, 1, 3, 4, 0, 4, 8, 0, 4, 3, 6, 8, 7, 6, 0, 9, 7, 5, 7, 2, 1, 1, 6,\n",
       "        8, 9, 4, 1, 5, 2, 2, 9, 0, 3, 9, 6, 7, 2, 0, 3, 5, 4, 3, 6, 5, 8, 9, 5,\n",
       "        4, 7, 4, 2, 7, 3, 4, 8, 9, 1, 3, 2, 8, 7, 9, 1, 8, 7, 4, 1, 3, 1, 1, 0,\n",
       "        2, 3, 9, 4, 9, 2, 1, 6, 8, 4, 1, 7, 4, 4, 9, 2, 5, 7, 2, 4, 4, 2, 1, 9,\n",
       "        7, 2, 8, 7, 6, 9, 2, 2, 3, 8, 1, 6, 5, 1, 1, 0, 2, 6, 4, 5, 8, 3, 1, 5,\n",
       "        1, 9, 2, 7, 4, 4, 4, 8, 1, 5, 8, 9, 5, 6, 7, 9, 9, 3, 7, 0, 9, 0, 6, 6,\n",
       "        2, 3, 9, 0, 7, 5, 4, 8, 0, 9, 4, 1, 2, 8, 7, 1, 2, 6, 1, 0, 3, 0, 1, 1,\n",
       "        8, 2, 0, 3, 9, 4, 0, 5, 0, 6, 1, 7, 7, 8, 1, 9, 2, 0, 5, 1, 2, 2, 7, 3,\n",
       "        5, 4, 9, 7, 1, 8, 3, 9, 6, 0, 3, 1, 1, 2, 6, 3, 5, 7, 6, 8, 7, 9, 5, 8,\n",
       "        5, 7, 6, 1, 1, 3, 1, 7, 5, 5, 5, 2, 5, 8, 7, 0, 9, 7, 7, 5, 0, 9, 0, 0,\n",
       "        8, 9, 2, 4, 8, 1, 6, 1, 6, 5, 1, 8, 3, 4, 0, 5, 5, 8, 3, 6, 2, 3, 9, 2,\n",
       "        1, 1, 5, 2, 1, 3, 2, 8, 7, 3, 7, 2, 4, 6, 9, 7, 2, 4, 2, 8, 1, 1, 3, 8,\n",
       "        4, 0, 6, 5, 9, 3, 0, 9, 2, 4, 7, 1, 2, 9, 4, 2, 6, 1, 8, 9, 0, 6, 6, 7,\n",
       "        9, 9, 8, 0, 1, 4, 4, 6, 7, 1, 5, 7, 0, 3, 5, 8, 4, 7, 1, 2, 5, 9, 5, 6,\n",
       "        7, 5, 4, 8, 8, 3, 6, 9, 7, 0, 7, 2, 7, 1, 1, 0, 7, 9, 2, 3, 7, 3, 2, 4,\n",
       "        1, 6, 2, 7, 5, 5, 7, 4, 0, 2, 6, 3, 6, 4, 0, 4, 2, 6, 0, 0, 0, 0, 3, 1,\n",
       "        6, 2, 2, 3, 1, 4, 1, 5, 4, 6, 4, 7, 2, 8, 7, 9, 2, 0, 5, 1, 4, 2, 8, 3,\n",
       "        2, 4, 1, 5, 4, 6, 0, 7, 9, 8, 4, 9, 8, 0, 1, 1, 0, 2, 2, 3, 2, 4, 4, 5,\n",
       "        8, 6, 5, 7, 7, 8, 8, 9, 7, 4, 7, 3, 2, 0, 8, 6, 8, 6, 1, 6, 8, 9, 4, 0,\n",
       "        9, 0, 4, 1, 5, 4, 7, 5, 3, 7, 4, 9, 8, 5, 8, 6, 3, 8, 6, 9, 9, 1, 8, 3,\n",
       "        5, 8, 6, 5, 9, 7, 2, 5, 0, 8, 5, 1, 1, 0, 9, 1, 8, 6, 7, 0, 9, 3, 0, 8,\n",
       "        8, 9, 6, 7, 8, 4, 7, 5, 9, 2, 6, 7, 4, 5, 9, 2, 3, 1, 6, 3, 9, 2, 2, 5,\n",
       "        6, 8, 0, 7, 7, 1, 9, 8, 7, 0, 9, 9, 4, 6, 2, 8, 5, 1, 4, 1, 5, 5, 1, 7,\n",
       "        3, 6, 4, 3, 2, 5, 6, 4, 4, 0, 4, 4, 6, 7, 9, 4, 3, 3, 8, 0, 0, 3, 2, 2,\n",
       "        9, 8, 2, 3, 7, 0, 1, 1, 0, 2, 3, 3, 8, 4, 3, 5, 7, 6, 4, 7, 7, 8, 5, 9,\n",
       "        7, 0, 3, 1, 1, 2, 4, 3, 4, 4, 7, 5, 9, 6, 0, 0, 7, 1, 4, 2, 7, 3, 6, 7,\n",
       "        5, 8, 4, 5, 5, 2, 7, 1, 1, 5, 6, 8, 5, 8, 4, 0, 7, 9, 9, 2, 9, 7, 7, 8,\n",
       "        7, 4, 2, 6, 9, 1, 7, 0, 6, 4, 2, 5, 7, 0, 7, 1, 0, 3, 7, 6, 5, 0, 6, 1,\n",
       "        5, 1, 7, 8, 5, 0, 3, 4, 7, 7, 5, 7, 8, 6, 9, 3, 8, 6, 1, 0, 9, 7, 1, 3,\n",
       "        0, 5, 6, 4, 4, 2, 4, 4, 3, 1, 7, 4, 6, 0, 3, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(X2).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9850)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((train_target.argmax(dim=1)==prediction(X2).argmax(dim=1))*1.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8500)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((test_target.argmax(dim=1)==prediction(X2_t1).argmax(dim=1))*1.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
